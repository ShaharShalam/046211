{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS3KcsAB3hdkx1qlaVaVij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tzurbar/046211/blob/main/Code/DINOv2_with_DORA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import shutil\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "d-XCayxMLQyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CktwJHmZLEQG"
      },
      "outputs": [],
      "source": [
        "# Define transformations for data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(root='/content/drive/My Drive/046211/Project/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = ImageFolder(root='/content/drive/My Drive/046211/Project/val', transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = ImageFolder(root='/content/drive/My Drive/046211/Project/test', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LowRankLayer(nn.Module):\n",
        "    def __init__(self, linear, rank, alpha, use_dora=True):\n",
        "        super().__init__()\n",
        "        # rank: controls the inner dimension of the matrices A and B; controls the number of additional parameters introduced by LoRA,\n",
        "        # a key factor in determining the balance between model adaptability and parameter efficiency.\n",
        "        # alpha: a scaling hyper-parameter applied to the output of the low-rank adaptation,\n",
        "        # controls the extent to which the adapted layer's output is allowed to influence the original output of the layer being adapted.\n",
        "\n",
        "        self.use_dora = use_dora\n",
        "        self.rank = rank  # low-rank\n",
        "        self.alpha = alpha  # scaling hyper-parameter\n",
        "        self.linear = linear\n",
        "        self.in_dim = linear.in_features\n",
        "        self.out_dim = linear.out_features\n",
        "\n",
        "        # weights\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(self.rank).float())\n",
        "        self.A = nn.Parameter(torch.randn(self.in_dim, self.rank) * std_dev)\n",
        "        self.B = nn.Parameter(torch.zeros(self.rank, self.out_dim))\n",
        "\n",
        "        if self.use_dora:\n",
        "            self.m = nn.Parameter(\n",
        "                self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
        "        else:\n",
        "            self.m = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        lora = self.A @ self.B  # combine LoRA matrices\n",
        "        if self.use_dora:\n",
        "            numerator = self.linear.weight + self.alpha * lora.T\n",
        "            denominator = numerator.norm(p=2, dim=0, keepdim=True)\n",
        "            directional_component = numerator / denominator\n",
        "            new_weight = self.m * directional_component\n",
        "            return F.linear(x, new_weight, self.linear.bias)\n",
        "        else:\n",
        "            # combine LoRA with orig. weights\n",
        "            combined_weight = self.linear.weight + self.alpha * lora.T\n",
        "            return F.linear(x, combined_weight, self.linear.bias)"
      ],
      "metadata": {
        "id": "sDy_n5FQLIQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=25, device='cuda'):\n",
        "    since = time.time()\n",
        "\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Backward pass + optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_dataloader.dataset)\n",
        "\n",
        "        print('train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "        train_loss_history.append(epoch_loss)\n",
        "        train_acc_history.append(epoch_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(val_dataloader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(val_dataloader.dataset)\n",
        "\n",
        "        print('val Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "        val_loss_history.append(epoch_loss)\n",
        "        val_acc_history.append(epoch_acc)\n",
        "\n",
        "        # Deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_acc_history, train_loss_history, val_acc_history, val_loss_history\n",
        "\n",
        "def evaluate_model(model, test_dataloader, criterion, device='cuda'):\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(test_dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(test_dataloader.dataset)\n",
        "\n",
        "    print('Test Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "eezCt_fkLJ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
        "print(model)\n",
        "# turn off gradients\n",
        "model.requires_grad_(False)\n",
        "rank = 4\n",
        "alpha = 8\n",
        "num_classes = 10\n",
        "model.blocks[-1].mlp.fc2 = LowRankLayer(model.blocks[-1].mlp.fc2, rank, alpha, use_dora=True)\n",
        "model.head = nn.Linear(model.norm.out_features, num_classes)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "eAmZpHexLLca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print and collect learnable paramters\n",
        "print(\"Params to learn:\")\n",
        "params_to_update = []  # override the initial list definition above\n",
        "for name,param in model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ],
      "metadata": {
        "id": "iiofMvROLNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper-parameters\n",
        "batch_size = 8\n",
        "num_epochs = 15\n",
        "model = model.to(device)\n",
        "# optimizer\n",
        "optimizer_ft = torch.optim.Adam(params_to_update, lr=0.0002)\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# train\n",
        "# Train and evaluate\n",
        "model, train_acc_history, train_loss_history, val_acc_history, val_loss_history = train_model(model, train_loader, val_loader, criterion, optimizer_ft, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "Zvo0og1KLOsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate_model(model, test_dataloader, criterion, device='cuda')\n",
        "print(f\"Test loss: {test_loss}\")\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "rLh6NqgoP5kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(train_acc_history, val_acc_history, train_loss_history, val_loss_history):\n",
        "    epochs = range(1, len(train_acc_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Subplot 1: Training and validation accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_acc_history, 'bo-', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc_history, 'ro-', label='Validation accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Subplot 2: Training and validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_loss_history, 'bo-', label='Training loss')\n",
        "    plt.plot(epochs, val_loss_history, 'ro-', label='Validation loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "58N5cx2vQCO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(train_acc_history, val_acc_history, train_loss_history, val_loss_history)"
      ],
      "metadata": {
        "id": "OjTaS-PmQuxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}